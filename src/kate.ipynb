{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "429a7c81-0585-49bf-9a59-b6dc44206dd3",
   "metadata": {},
   "source": [
    "## Overview  \n",
    "&emsp;Adaptive Boosting (AdaBoost), first introduced by Freund and Schapire in 1997, is a supervised learning algorithm that combines many weak classifiers into a strong classifier. In this case, a weak classifier is any model that can perform slightly better than random guessing. AdaBoost builds the final model sequentially, where each weak learner is trained on a weighted dataset, and misclassified points from previous iterations are given higher weight in the sequential iterations. In this way, the later learners are focused more on the difficult and misclassified data points, and the ensemble learning continually improves throughout training.\n",
    " \n",
    "&emsp;At each iteration, AdaBoost fits a weak learner, evaluates its weighted classification error, and assigns that learner a weight based on its performance. The final prediction is a weighted vote over all weak learners. The most common weak learner used in AdaBoost are decision stumps, which are used in this project. Decision stumps are decision trees with a depth of one, and they are used as they are fast to train. If any weak learner is found to have a classification error of 50%, it is omitted from the ensemble as that learner is no better than random guessing.  \n",
    "\n",
    "&emsp;AdaBoost has several advantages as a model. Conceptually, as it is a combination of weak learners, it is simple and requires very few hyperparameters. The algorithm can achieve high accuracy with weak base models and often avoid overfitting, especially when using decision stumps. In other cases, it is also model-agnostic, and any classifier capable of handling weighted data can serve as the base learner.\n",
    "\n",
    "&emsp;However, there are important limitations to consider when choosing AdaBoost. As stated above, there is an emphasis on misclassified points in each iteration of boosting, meanign it is highly sensitive to nosie and outliers. This means, if there is a mislabeled point that recieves a high weight repeatedly, the ensemble will focus too much on this point and trying to correct it, and this reduces generalization. AdaBoost can lack interpretability due to the ensemble nature, the final model and decision boundary is complex, and the process of iterative weighting could be difficult to explain in simple rules. Also, even though AdaBoost is resistant to overfitting, the performance of the model can decrease when the number of iterations is very large.  \n",
    "\n",
    "---\n",
    "**Representation**  \n",
    "In binary classification with labels $y \\in \\{-1, +1\\}$, each weak learner $h_t(x)$ outputs a prediction in the same set. AdaBoost combines $T$ weak learners, weighted by their important $w_t$:\n",
    "\n",
    "$$\n",
    "F(x) = \\sum_{t=1}^{T} w_t h_t(x)\n",
    "$$  \n",
    "\n",
    "To convert this score into a single-numbered prediciton, AdaBoost uses the sign function:\n",
    "\n",
    "$$\n",
    "h_s(x) = \\mathrm{sign}(F(x))\n",
    "$$  \n",
    "\n",
    "Therefore, the representation is an additive linear model of weak classifiers, where each classifier's contribution is scaled by $\\alpha_t$.  \n",
    "\n",
    "---\n",
    "**Loss**  \n",
    "AdaBoost minimizes the exponential loss:\n",
    "$$\n",
    "L = \\sum_{i=1}^{n} \\exp(-y_i F(x_i))\n",
    "$$\n",
    "Where $F(x_i) = \\sum_{t=1}^{T} w_t h_t(x_i)$\n",
    "This loss penalizes wrong predictions more heavily. If $y_i F(x_i) < 0$, the eponential term becomes large, increasing the weight assigned to that sample in the next iteration, $D_i^{(t+1)}. This adaptiveness is the key property of AdaBoost.  \n",
    "\n",
    "---\n",
    "**Optimizer**  \n",
    "AdaBoost uses a stage-wise, greedy optimization strategy. Instead of optimizing all parameters jointly, it adds one weak learner at a time, choosing $h_t$ and $w_t$ that reduce the exponential loss the most at each round. At iteration $t$ with weights $D_i^{(t)}$ the weighted classification error is:\n",
    "$$\n",
    "\\epsilon_t = \\sum_{i=1}^{m} D_i^{(t)} \\mathbf{1}_{[y_i \\ne h_t(x_i)]}\n",
    "$$\n",
    "The learner's weight is then:\n",
    "$$\n",
    "w_t = \\frac{1}{2} \\log\\left( \\frac{1}{\\epsilon_t} - 1 \\right)\n",
    "$$\n",
    "Sample weights are updated using:\n",
    "$$\n",
    "D_i^{(t+1)} = \\frac{D_i^{(t)} exp(-w_ty_ih_t(x_i))}{\\sum_{j=1}^mD_j^{(t)}exp(-w_ty_jh_t(x_j))}\n",
    "$$\n",
    "for all  $i = 1,...,m$  \n",
    "and are normalized so that:\n",
    "$$\n",
    "\\sum_{i=1}^m D_i^{(t+1)} = 1\n",
    "$$\n",
    "The final classifier is obtained as the sign of the weighted sum of all weak learners:\n",
    "$$\n",
    "h_s(x) = \\mathrm{sign}(\\sum_{t=1}^T w_th_t(x))\n",
    "$$\n",
    "\n",
    "---\n",
    "**Pseudocode**  \n",
    "**Input:**   \n",
    "training set S ${(x_i, y_i)},...,{(x_m, y_m)}$  \n",
    "weak learner $WL$  \n",
    "number of rounds $T$  \n",
    "**Initialize:**    \n",
    "example weights: $D^{(1)} = (\\frac{1}{m},...\\frac{1}{m})$\n",
    "\n",
    "**For** $t = 1$ to $T$:  \n",
    "&emsp;Train weak learner $h_t = WL(D^{(t)}, S)$  \n",
    "&emsp;Compute weighted error:  \n",
    "&emsp;&emsp;$\\epsilon_t = \\sum_{i=1}^m D_i^{(t)} \\mathbf{1}_{[y_i \\ne h_t(x_i)]}$  \n",
    "&emsp;Compute learner weight:  \n",
    "&emsp;&emsp;$w_t = \\frac{1}{2} \\log\\left( \\frac{1}{\\epsilon_t} - 1 \\right)$  \n",
    "&emsp;Update example weights:  \n",
    "&emsp;&emsp;$D_i^{(t+1)} =  \\frac{D_i^{(t)} exp(-w_ty_ih_t(x_i))}{\\sum_{j=1}^mD_j^{(t)}exp(-w_ty_jh_t(x_j))}$ for all $i = 1,...,m$  \n",
    "&emsp;Normalize weights so that:  \n",
    "&emsp;&emsp;$\\sum_{i=1}^m D_i^{(t+1)} = 1$  \n",
    "**Output** the hypothesis $h_s(x) = \\mathrm{sign}(\\sum_{t=1}^T w_th_t(x))$\n",
    "\n",
    "---\n",
    "**References**  \n",
    "Freund, Y. & Schapire, R.E., 1997. A Decision‑Theoretic Generalization of On‑Line Learning and an Application to Boosting. Journal of Computer and System Sciences, 55(1), pp.119–139. https://doi.org/10.1006/jcss.1997.1504  \n",
    "GeeksforGeeks, 2025. AdaBoost in Machine Learning. [online] Available at: https://www.sciencedirect.com/science/article/pii/S002200009791504X\n",
    " [Accessed 5 December 2025].  \n",
    "Schapire, R.E., 2013. Explaining AdaBoost. In: B. Schölkopf, Z. Luo and V. Vovk, eds. Empirical Inference: Festschrift in Honour of Vladimir N. Vapnik. Berlin: Springer, pp.37–52. DOI: 10.1007/978‑3‑642‑41136‑6_5.  \n",
    "de Giorgio, A., 2023. Systematic review of class imbalance problems in machine learning and deep learning solutions in manufacturing. Expert Systems with Applications, 229, p.120193. Available at: https://www.sciencedirect.com/science/article/pii/S0278612523002157\n",
    " [Accessed 5 December 2025]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
